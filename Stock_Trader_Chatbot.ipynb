{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6Vj9/9vo8mu6i0B0R9EF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaewon078/Stock-Trader-Chatbot/blob/main/Stock_Trader_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Stock Trader Chatbot by Jaewon Lee</h1>\n"
      ],
      "metadata": {
        "id": "owr3ghEpB0pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> General Information: </h2>\n",
        "\n",
        "* Model: GPT 4o\n",
        "* Colab Environment: CPU\n"
      ],
      "metadata": {
        "id": "WTrfMks-B-TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up libraries and the model"
      ],
      "metadata": {
        "id": "Ny1y6Rr9E6PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install -q openai==1.44.1\n",
        "!pip install -q gradio==4.43.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2qJfLTMCQZU",
        "outputId": "7e6fbcf3-46d7-4e7c-e413-44f199727557"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/373.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/373.5 kB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import openai\n",
        "import random\n",
        "import gradio"
      ],
      "metadata": {
        "id": "2xMBZqYPCj7l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert OpenAI API Key here (you will be prompted)\n",
        "# API keys are available at https://platform.openai.com/account/api-keys\n",
        "from getpass import getpass\n",
        "openai.api_key=getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# We are using GPT-4o as our model\n",
        "model = \"gpt-4o\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHIHJmD-C75b",
        "outputId": "045df381-328e-422a-f5b8-6e1cc011ffa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the context so the model acts like an experienced stock trader\n",
        "\n",
        "context = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': \"\"\"\n",
        "You are an experienced stock trader with over 20 years of experience in the financial markets. Your expertise spans various sectors including technology, healthcare, energy, and consumer goods. You have a deep understanding of fundamental and technical analysis, market trends, and economic indicators.\n",
        "\n",
        "Your role is to provide advice and insights on stock trading to users. This includes:\n",
        "1. Analyzing specific stocks or market sectors when requested\n",
        "2. Explaining trading strategies and concepts\n",
        "3. Discussing current market trends and their potential impacts\n",
        "4. Offering general advice on portfolio management and risk mitigation\n",
        "\n",
        "When giving advice, always:\n",
        "1. Emphasize that your advice is for informational purposes only and should not be considered as financial advice\n",
        "2. Encourage users to do their own research and consult with a licensed financial advisor before making investment decisions\n",
        "3. Explain the reasoning behind your analysis or recommendations\n",
        "4. Discuss both potential upsides and risks associated with any strategy or stock\n",
        "5. Use clear, concise language and explain any technical terms you use\n",
        "\n",
        "Remember to stay up-to-date with the latest market news and trends. If asked about specific recent events, you can provide general analysis based on typical market behavior, but remind the user that you don't have real-time data and they should verify current information.\n",
        "\n",
        "Never provide guarantees about stock performance or make promises about returns. Always stress the importance of diversification and risk management in investing.\n",
        "\n",
        "Respond directly to user queries without unnecessary affirmations or filler phrases. Be concise but thorough, offering to elaborate if further information may be helpful.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': \"Hello! I'm your experienced stock trading assistant. How can I help you with your investment queries today?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "pTQ_hxn2DIHM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the logic for maintaining the memory of the conversation"
      ],
      "metadata": {
        "id": "D5oul8kPFBgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates the next AI response in a conversation using OpenAI's Chat API\n",
        "#\n",
        "# Parameters:\n",
        "# messages: List of message dictionaries representing the conversation history\n",
        "#           Each dictionary should have 'role' (system/user/assistant) and 'content' keys\n",
        "# temperature: Float between 0 and 2. Lower values make output more focused and deterministic\n",
        "#\n",
        "# Returns:\n",
        "# str: The content of the AI's response message\n",
        "\n",
        "def continue_conversation(messages, temperature=0):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "2QGR8HvwEZAf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Gradio"
      ],
      "metadata": {
        "id": "t44ZUL3uFzUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processes a new user message in a Gradio chat interface and generates an AI response\n",
        "#\n",
        "# This function is designed to work with Gradio's chat interface. It takes the current\n",
        "# user message and the conversation history, formats them into the expected structure\n",
        "# for the OpenAI API, and then generates the next AI response.\n",
        "#\n",
        "# Parameters:\n",
        "# message: str - The current user message to be processed\n",
        "# history: list - A list of tuples, each containing (user_message, assistant_message)\n",
        "#                 representing the chat history\n",
        "#\n",
        "# Returns:\n",
        "# str - The AI's response to the current message\n",
        "\n",
        "def gradio_chat(message, history):\n",
        "    # Initialize the chat history with the predefined context\n",
        "    history_chat = context\n",
        "\n",
        "    # Add each user and AI message to the chat history\n",
        "    for user, assistant in history:\n",
        "        history_chat.append({\"role\":\"user\", \"content\":user})\n",
        "        history_chat.append({\"role\":\"assistant\", \"content\":assistant})\n",
        "\n",
        "    # Add the current user message to the chat history\n",
        "    history_chat.append({\"role\":\"user\", \"content\":message})\n",
        "\n",
        "    # Generate and return the AI's response using the complete chat history\n",
        "    return continue_conversation(history_chat, 0)"
      ],
      "metadata": {
        "id": "dVMxBDEZF1dh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customized gradio textbox\n",
        "InputText = gradio.Textbox(label=\"Send\", info=\"Your response here.\", scale= 6)"
      ],
      "metadata": {
        "id": "oqEDwYDPGsfL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Result"
      ],
      "metadata": {
        "id": "ZRKfKxBSG_an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradio.ChatInterface(gradio_chat,\n",
        "                 textbox=InputText,\n",
        "                 retry_btn=None,\n",
        "                 undo_btn=None,\n",
        "                 title=\"Stock Trader Chatbot\",\n",
        "                submit_btn=\"Send\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "hto5MEoQHBZb",
        "outputId": "8ea8e812-2409-470e-e3d1-d826dad18112"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1bd44908fc41fb79b3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1bd44908fc41fb79b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}